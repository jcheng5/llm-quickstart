[
  {
    "objectID": "quickstart.html#setup",
    "href": "quickstart.html#setup",
    "title": "LLM Quick Start",
    "section": "Setup",
    "text": "Setup\nOption 1: Cloud\n\nLog in to https://workbench.posit.it/ and start a session in your IDE of choice\n\nWe recommend RStudio for R, and Positron for R or Python\n\nClone https://github.com/jcheng5/llm-quickstart\nOpen llm-quickstart as a Project (RStudio) or Folder (Positron) in your IDE\nGrab your OpenAI/Anthropic API keys; see the thread in #hackathon-20\n\nOption 2: Local\n\nClone https://github.com/jcheng5/llm-quickstart\nGrab your OpenAI/Anthropic API keys; see the thread in #hackathon-20\nFor R: install.packages(c(\"ellmer\", \"shinychat\", \"dotenv\", \"shiny\", \"paws.common\", \"magick\", \"beepr\"))\nFor Python: pip install -r requirements.txt"
  },
  {
    "objectID": "quickstart.html#framing-llms",
    "href": "quickstart.html#framing-llms",
    "title": "LLM Quick Start",
    "section": "Framing LLMs",
    "text": "Framing LLMs\n\n\nOur focus: Practical, actionable information\n\nOften, just enough knowledge so you know what to search for (or better yet, what to ask an LLM)\n\nWe will treat LLMs as black boxes\nDon’t focus on how they work (yet)\n\nLeads to bad intuition about their capabilities\nBetter to start with a highly empirical approach"
  },
  {
    "objectID": "quickstart.html#llm-conversations-are-http-requests",
    "href": "quickstart.html#llm-conversations-are-http-requests",
    "title": "LLM Quick Start",
    "section": "LLM Conversations are HTTP Requests",
    "text": "LLM Conversations are HTTP Requests\n\n\nEach interaction is a separate HTTP API request\nThe API server is entirely stateless (despite conversations being inherently stateful!)"
  },
  {
    "objectID": "quickstart.html#example-conversation",
    "href": "quickstart.html#example-conversation",
    "title": "LLM Quick Start",
    "section": "Example Conversation",
    "text": "Example Conversation\n\n“What’s the capital of the moon?”\n\n\"There isn't one.\"\n\n“Are you sure?”\n\n\"Yes, I am sure.\""
  },
  {
    "objectID": "quickstart.html#example-request",
    "href": "quickstart.html#example-request",
    "title": "LLM Quick Start",
    "section": "Example Request",
    "text": "Example Request\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"gpt-4.1\",\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a terse assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of the moon?\"}\n    ]\n}'\n\nSystem prompt: behind-the-scenes instructions and information for the model\nUser prompt: a question or statement for the model to respond to"
  },
  {
    "objectID": "quickstart.html#example-response-abridged",
    "href": "quickstart.html#example-response-abridged",
    "title": "LLM Quick Start",
    "section": "Example Response (abridged)",
    "text": "Example Response (abridged)\n{\n  \"choices\": [{\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"The moon does not have a capital. It is not inhabited or governed.\",\n    },\n    \"finish_reason\": \"stop\"\n  }],\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"completion_tokens\": 12,\n    \"total_tokens\": 21,\n    \"completion_tokens_details\": {\n      \"reasoning_tokens\": 0\n    }\n  }\n}"
  },
  {
    "objectID": "quickstart.html#example-request-1",
    "href": "quickstart.html#example-request-1",
    "title": "LLM Quick Start",
    "section": "Example Request",
    "text": "Example Request\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"gpt-4.1\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a terse assistant.\"},\n      {\"role\": \"user\", \"content\": \"What is the capital of the moon?\"},\n      {\"role\": \"assistant\", \"content\": \"The moon does not have a capital. It is not inhabited or governed.\"},\n      {\"role\": \"user\", \"content\": \"Are you sure?\"}\n    ]\n}'"
  },
  {
    "objectID": "quickstart.html#example-response-abridged-1",
    "href": "quickstart.html#example-response-abridged-1",
    "title": "LLM Quick Start",
    "section": "Example Response (abridged)",
    "text": "Example Response (abridged)\n{\n  \"choices\": [{\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"Yes, I am sure. The moon has no capital or formal governance.\"\n    },\n    \"finish_reason\": \"stop\"\n  }],\n  \"usage\": {\n    \"prompt_tokens\": 52,\n    \"completion_tokens\": 15,\n    \"total_tokens\": 67,\n    \"completion_tokens_details\": {\n      \"reasoning_tokens\": 0\n    }\n  }\n}"
  },
  {
    "objectID": "quickstart.html#tokens",
    "href": "quickstart.html#tokens",
    "title": "LLM Quick Start",
    "section": "Tokens",
    "text": "Tokens\n\n\nFundamental units of information for LLMs\nWords, parts of words, or individual characters\n\n“hello” → 1 token\n“unconventional” → 3 tokens: un|con|ventional\n4K video frame at full res → 6885 tokens\n\nExample with OpenAI Tokenizer\nImportant for:\n\nModel input/output limits\nAPI pricing is usually by token (see calculator)"
  },
  {
    "objectID": "quickstart.html#instructions",
    "href": "quickstart.html#instructions",
    "title": "LLM Quick Start",
    "section": "Instructions",
    "text": "Instructions\nOpen and run one of these options:\n\n01-basics.R\n\nor 01-basics-bedrock.R for cloud\n\n01-basics-openai.py (low level library)\n01-basics-langchain.py (high level framework)\n01-basics-chatlas.py (high level framework, similar to R)\n\nor 01-basics-chatlas-bedrock.py for cloud\n\n\nIf it errors, now is the time to debug.\nIf it works, study the code and try to understand how it maps to the low-level HTTP descriptions we just went through."
  },
  {
    "objectID": "quickstart.html#summary",
    "href": "quickstart.html#summary",
    "title": "LLM Quick Start",
    "section": "Summary",
    "text": "Summary\n\nA message is an object with a role (“system”, “user”, “assistant”) and a content string\nA chat conversation is a growing list of messages\nThe OpenAI chat API is a stateless HTTP endpoint: takes a list of messages as input, returns a new message as output"
  },
  {
    "objectID": "quickstart.html#shiny-for-r",
    "href": "quickstart.html#shiny-for-r",
    "title": "LLM Quick Start",
    "section": "Shiny for R",
    "text": "Shiny for R\n{shinychat} package\nhttps://github.com/posit-dev/shinychat\n\nDesigned to be used with ellmer\nEllmer Assistant is quite good for getting started"
  },
  {
    "objectID": "quickstart.html#shiny-for-python",
    "href": "quickstart.html#shiny-for-python",
    "title": "LLM Quick Start",
    "section": "Shiny for Python",
    "text": "Shiny for Python\nCreating chatbots in Shiny for Python\n\nShiny Assistant on the web can’t help you with ui.Chat for data privacy reasons, so instead…"
  },
  {
    "objectID": "quickstart.html#shiny-assistant-for-vs-code-and-positron",
    "href": "quickstart.html#shiny-assistant-for-vs-code-and-positron",
    "title": "LLM Quick Start",
    "section": "Shiny Assistant for VS Code and Positron",
    "text": "Shiny Assistant for VS Code and Positron"
  },
  {
    "objectID": "quickstart.html#installation",
    "href": "quickstart.html#installation",
    "title": "LLM Quick Start",
    "section": "Installation",
    "text": "Installation"
  },
  {
    "objectID": "quickstart.html#shiny-assistant-requirements",
    "href": "quickstart.html#shiny-assistant-requirements",
    "title": "LLM Quick Start",
    "section": "Shiny Assistant requirements",
    "text": "Shiny Assistant requirements\n\nIn VS Code, requires Copilot subscription\nIn Positron, requires:\n\nAnthropic or OpenAI API key\nEnable Positron Assistant. Instructions"
  },
  {
    "objectID": "quickstart.html#querychat",
    "href": "quickstart.html#querychat",
    "title": "LLM Quick Start",
    "section": "{querychat}",
    "text": "{querychat}\n\nhttps://github.com/posit-dev/querychat (R and Python)"
  },
  {
    "objectID": "quickstart.html#other-python-frameworks",
    "href": "quickstart.html#other-python-frameworks",
    "title": "LLM Quick Start",
    "section": "Other Python frameworks",
    "text": "Other Python frameworks\n\nStreamlit has an excellent chat component with a nice LangChain integration\nGradio has a chat component that is extremely easy to use"
  },
  {
    "objectID": "quickstart.html#what-is-tool-calling",
    "href": "quickstart.html#what-is-tool-calling",
    "title": "LLM Quick Start",
    "section": "What is Tool Calling?",
    "text": "What is Tool Calling?\n\n\nAllows LLMs to interact with other systems\nSounds complicated? It isn’t!\nSupported by most of the newest LLMs, but not all"
  },
  {
    "objectID": "quickstart.html#how-it-works",
    "href": "quickstart.html#how-it-works",
    "title": "LLM Quick Start",
    "section": "How It Works",
    "text": "How It Works\n\n\nNot like this - with the assistant executing stuff\nYes like this\n\nUser asks assistant a question; includes metadata for available tools\nAssistant asks the user to invoke a tool, passing its desired arguments\nUser invokes the tool, and returns the output to the assistant\nAssistant incorporates the tool’s output as additional context for formulating a response"
  },
  {
    "objectID": "quickstart.html#how-it-works-1",
    "href": "quickstart.html#how-it-works-1",
    "title": "LLM Quick Start",
    "section": "How It Works",
    "text": "How It Works\nAnother way to think of it:\n\nThe client can perform tasks that the assistant can’t do\nTools put control into the hands of the assistant—it decides when to use them, and what arguments to pass in, and what to do with the results\nHaving an “intelligent-ish” coordinator of tools is a surprisingly general, powerful capability!"
  },
  {
    "objectID": "quickstart.html#openai-models",
    "href": "quickstart.html#openai-models",
    "title": "LLM Quick Start",
    "section": "OpenAI models",
    "text": "OpenAI models\n\nGPT-4.1: good general purpose model, 1 million token context length\n\nGPT-4.1-mini and GPT-4.1-nano are faster, cheaper, and dumber versions\n\no3: reasoning model; better for complex math and coding, but much slower and more expensive\no4-mini: faster and cheaper reasoning model, not as good as o3 but cheaper than GPT-4.1\nAPI access via OpenAI or Azure\nTakeaway: Good models for general purpose use"
  },
  {
    "objectID": "quickstart.html#anthropic-models",
    "href": "quickstart.html#anthropic-models",
    "title": "LLM Quick Start",
    "section": "Anthropic models",
    "text": "Anthropic models\n\nClaude Sonnet 4: good general purpose model, best for code generation\n\nClaude Sonnet 3.7 and 3.5 are both still excellent\n\nClaude Opus 4: even stronger than Sonnet 4 (supposedly), but more expensive and slower\nClaude 3.5 Haiku: Faster, cheaper but not cheap enough\nAPI access via Anthropic or AWS Bedrock (instructions for using Bedrock at Posit)\nTakeaway: Best model for code generation"
  },
  {
    "objectID": "quickstart.html#google-models",
    "href": "quickstart.html#google-models",
    "title": "LLM Quick Start",
    "section": "Google models",
    "text": "Google models\n\nGemini 2.5 Flash: 1 million token context length, very fast\nGemini 2.5 Pro: 1 million token context length, smarter than Flash\nTakeaway: Competitive with OpenAI and Anthropic"
  },
  {
    "objectID": "quickstart.html#llama-models",
    "href": "quickstart.html#llama-models",
    "title": "LLM Quick Start",
    "section": "Llama models",
    "text": "Llama models\n\nOpen weights: you can download the model\nCan run locally, for example with Ollama\nLlama 3.1 405b: text, 229GB. Not quite as smart as best closed models.\nLlama 3.2 90b: text+vision, 55GB\nLlama 3.2 11b: text+vision, 7.9GB (can run comfortably on Macbook)\nAPI access via OpenRouter, Groq, AWS Bedrock, others\nTakeaway: OK models if you want to keep all information on premises."
  },
  {
    "objectID": "quickstart.html#the-problem",
    "href": "quickstart.html#the-problem",
    "title": "LLM Quick Start",
    "section": "The problem",
    "text": "The problem\n\nYou want to customize how the LLM responds\nLLM doesn’t know your specific information\n\nSome solutions\n\nPrompt engineering\nRetrieval-Augmented Generation\nAgentic search\nFine tuning"
  },
  {
    "objectID": "quickstart.html#prompt-engineering-directing-behavioroutput",
    "href": "quickstart.html#prompt-engineering-directing-behavioroutput",
    "title": "LLM Quick Start",
    "section": "Prompt engineering: Directing behavior/output",
    "text": "Prompt engineering: Directing behavior/output\n\n“Respond with just the minimal information necessary.”\n“Think through this step-by-step.”\n“Carefully read and follow these instructions…”\n“If the user asks a question related to data processing, produce R code to accomplish that task.”\n“Be careful to only provide answers that you are sure about. If you are uncertain about an answer, say so.”"
  },
  {
    "objectID": "quickstart.html#prompt-engineering-using-examples-to-guide-behavior",
    "href": "quickstart.html#prompt-engineering-using-examples-to-guide-behavior",
    "title": "LLM Quick Start",
    "section": "Prompt engineering: Using examples to guide behavior",
    "text": "Prompt engineering: Using examples to guide behavior\n\nGoal: Extract ingredient list from recipe and return in a structured format.\nExample user input:\nIn a large bowl, cream together 1 cup of softened unsalted butter and ½ cup of white sugar until smooth. Beat in 1 egg and 1 teaspoon of vanilla extract. Gradually stir in 2 cups of all-purpose flour until the dough forms. Finally, fold in 1 cup of semisweet chocolate chips. Drop spoonfuls of dough onto an ungreased baking sheet and bake at 350°F (175°C) for 10-12 minutes, or until the edges are lightly browned. Let the cookies cool on the baking sheet for a few minutes before transferring to a wire rack to cool completely. Enjoy!"
  },
  {
    "objectID": "quickstart.html#system-prompt-1-just-instructions",
    "href": "quickstart.html#system-prompt-1-just-instructions",
    "title": "LLM Quick Start",
    "section": "System prompt 1: just instructions",
    "text": "System prompt 1: just instructions\nThe user input contains a recipe. Extract a list of ingredients and return it in JSON format.\nAssistant response:\n{\n  \"ingredients\": [\n    \"1 cup of softened unsalted butter\",\n    \"½ cup of white sugar\",\n    \"1 egg\",\n    \"1 teaspoon of vanilla extract\",\n    \"2 cups of all-purpose flour\",\n    \"1 cup of semisweet chocolate chips\"\n  ]\n}"
  },
  {
    "objectID": "quickstart.html#system-prompt-2-detailed-description-of-output",
    "href": "quickstart.html#system-prompt-2-detailed-description-of-output",
    "title": "LLM Quick Start",
    "section": "System prompt 2: detailed description of output",
    "text": "System prompt 2: detailed description of output\nThe user input contains a recipe. Extract a list of ingredients and return it in JSON format. It should be an array of objects, where each object has keys `ingredient`, `quantity`, and `unit`. Put each object on one line of output.\nAssistant response:\n[\n    {\"ingredient\": \"unsalted butter\", \"quantity\": 1, \"unit\": \"cup\"},\n    {\"ingredient\": \"white sugar\", \"quantity\": 1/2, \"unit\": \"cup\"},\n    {\"ingredient\": \"egg\", \"quantity\": 1, \"unit\": \"large\"},\n    {\"ingredient\": \"vanilla extract\", \"quantity\": 1, \"unit\": \"teaspoon\"},\n    {\"ingredient\": \"all-purpose flour\", \"quantity\": 2, \"unit\": \"cups\"},\n    {\"ingredient\": \"semisweet chocolate chips\", \"quantity\": 1, \"unit\": \"cup\"}\n]"
  },
  {
    "objectID": "quickstart.html#system-prompt-3-one-shot-example",
    "href": "quickstart.html#system-prompt-3-one-shot-example",
    "title": "LLM Quick Start",
    "section": "System prompt 3: one-shot example",
    "text": "System prompt 3: one-shot example\nThe user input contains a recipe. Extract a list of ingredients and return it in JSON format.\n\nExample Output:\n\n```json\n[\n    { \"ingredient\": \"Flour\", \"quantity\": 1, \"unit\": \"cup\" },\n    { \"ingredient\": \"Vegetable oil\", \"quantity\": 0.5, \"unit\": \"tsp\" },\n    { \"ingredient\": \"Onion\", \"quantity\": 1, \"unit\": null },\n]\n```\nAssistant response:\n[\n    { \"ingredient\": \"Unsalted butter\", \"quantity\": 1, \"unit\": \"cup\" },\n    { \"ingredient\": \"White sugar\", \"quantity\": 0.5, \"unit\": \"cup\" },\n    { \"ingredient\": \"Egg\", \"quantity\": 1, \"unit\": null },\n    { \"ingredient\": \"Vanilla extract\", \"quantity\": 1, \"unit\": \"teaspoon\" },\n    { \"ingredient\": \"All-purpose flour\", \"quantity\": 2, \"unit\": \"cups\" },\n    { \"ingredient\": \"Semisweet chocolate chips\", \"quantity\": 1, \"unit\": \"cup\" }\n]"
  },
  {
    "objectID": "quickstart.html#adding-contextknowledge-to-prompt",
    "href": "quickstart.html#adding-contextknowledge-to-prompt",
    "title": "LLM Quick Start",
    "section": "Adding context/knowledge to prompt",
    "text": "Adding context/knowledge to prompt\n\nAdd documentation files to prompt\nAdd positive examples (negative examples don’t work well)\nDocs must fit in context window\nExamples\n\nEllmer assistant uses README files in prompt\nSidebot\nFastHTML LLM prompt"
  },
  {
    "objectID": "quickstart.html#rag-retrieval-augmented-generation",
    "href": "quickstart.html#rag-retrieval-augmented-generation",
    "title": "LLM Quick Start",
    "section": "RAG: Retrieval Augmented Generation",
    "text": "RAG: Retrieval Augmented Generation\n\nUseful when documents don’t fit into context window\nSteps:\n\nUser sends query to app: “How do I …?”\nApp retrieves relevant chunks of text via search\nApp sends text and query to LLM\n\n&lt;chunk 1&gt;, &lt;chunk 2&gt;, &lt;chunk 3&gt;. How do I …?\n\nLLM responds with answer\n\nSearch method typically a semantic instead of keyword search, using vector DB\nLLM will only know about chunks that were retrieved; does not “know” entire corpus\nIn general, prompt stuffing works better, if docs fit in context window"
  },
  {
    "objectID": "quickstart.html#agentic-search",
    "href": "quickstart.html#agentic-search",
    "title": "LLM Quick Start",
    "section": "Agentic search",
    "text": "Agentic search\n\nSimilar to RAG:\n\nExtra information is provided to LLM\n\nDifferent from RAG:\n\nApplication does not search documents and send to LLM along with user prompt\nUser prompt is sent to LLM, then LLM uses a tool to search for relevant documents"
  },
  {
    "objectID": "quickstart.html#fine-tuning",
    "href": "quickstart.html#fine-tuning",
    "title": "LLM Quick Start",
    "section": "Fine tuning",
    "text": "Fine tuning\n\nUpdate weights for an existing model with new information\nNot all models can be fine-tuned\nData must be provided in chat conversation format, with query and response\n\nCan’t just feed it documents – this makes fine-tuning more difficult in practice\n\nSupposedly not very effective unless you have a lot of training data"
  },
  {
    "objectID": "quickstart.html#takeaways",
    "href": "quickstart.html#takeaways",
    "title": "LLM Quick Start",
    "section": "Takeaways",
    "text": "Takeaways\n\nFirst try prompting, then RAG or agentic search, and then fine tuning.\nOther resources\n\nOpenAI’s prompt engineering guide\nAnthropic’s prompt engineering guide\nFine-tuning vs. RAG article"
  },
  {
    "objectID": "quickstart.html#going-beyond-chat",
    "href": "quickstart.html#going-beyond-chat",
    "title": "LLM Quick Start",
    "section": "Going beyond chat",
    "text": "Going beyond chat\n\nStructured output can be easily consumed by code: JSON, YAML, CSV, etc.\nUnstructured output cannot: text, images, etc.\n\nLLMs are good at generating unstructured output, but with a little effort, you can get structured output as well."
  },
  {
    "objectID": "quickstart.html#several-techniques-choose-one",
    "href": "quickstart.html#several-techniques-choose-one",
    "title": "LLM Quick Start",
    "section": "Several techniques (choose one)",
    "text": "Several techniques (choose one)\n\nPost-processing: Use a regular expression to extract structured data from the unstructured output (e.g. /```json\\n(.*?)\\n```/)\nSystem prompt: Simply ask the LLM to output structured data. Be clear about what specific format you want, and provide examples—it really helps!\nStructured Output: GPT-4.1 and GPT-4.1-mini have a first-class Structured Output feature: outputs strictly adhere to a JSON schema you write. (Docs: openai, LangChain)\nTool calling: Create a tool to receive your output, e.g., set_result(object), where its implementation sets some variable. (Works great for ellmer.)\nLangChain: Has its own abstractions for parsing unstructured output\n\nAsk #hackathon-20 for help if you’re stuck! (Or ask ChatGPT/Claude to make an example.)"
  },
  {
    "objectID": "quickstart.html#using-images-as-input",
    "href": "quickstart.html#using-images-as-input",
    "title": "LLM Quick Start",
    "section": "Using images as input",
    "text": "Using images as input\n\nModern models are pretty good at this—but this frontier is especially jagged\nCan understand both photographs and plots\nExamples for R and Chatlas are in your repo as 05-vision*\nSee docs for LangChain multimodal, OpenAI vision"
  }
]