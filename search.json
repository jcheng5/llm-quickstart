[
  {
    "objectID": "apps/tool-chat/prompt.html",
    "href": "apps/tool-chat/prompt.html",
    "title": "",
    "section": "",
    "text": "You are an assistant which can convert units and perform basic arithmetic with units."
  },
  {
    "objectID": "quickstart.html#framing-llms",
    "href": "quickstart.html#framing-llms",
    "title": "LLM Quick Start",
    "section": "Framing LLMs",
    "text": "Framing LLMs\n\n\nOur focus: Practical, actionable information\n\nOften, just enough knowledge so you know what to search for (or better yet, what to ask an LLM)\n\nWe will treat LLMs as black boxes\nDon’t focus on how they work (yet)\n\nLeads to bad intuition about their capabilities\nBetter to start with a highly empirical approach"
  },
  {
    "objectID": "quickstart.html#llm-conversations-are-http-requests",
    "href": "quickstart.html#llm-conversations-are-http-requests",
    "title": "LLM Quick Start",
    "section": "LLM Conversations are HTTP Requests",
    "text": "LLM Conversations are HTTP Requests\n\n\nEach interaction is a separate HTTP API request\nThe API server is entirely stateless (despite conversations being inherently stateful!)"
  },
  {
    "objectID": "quickstart.html#example-conversation",
    "href": "quickstart.html#example-conversation",
    "title": "LLM Quick Start",
    "section": "Example Conversation",
    "text": "Example Conversation\n\n“What’s the capital of the moon?”\n\n\"There isn't one.\"\n\n“Are you sure?”\n\n\"Yes, I am sure.\""
  },
  {
    "objectID": "quickstart.html#example-request",
    "href": "quickstart.html#example-request",
    "title": "LLM Quick Start",
    "section": "Example Request",
    "text": "Example Request\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a terse assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of the moon?\"}\n    ]\n}'\n\nSystem prompt: behind-the-scenes instructions and information for the model\nUser prompt: a question or statement for the model to respond to"
  },
  {
    "objectID": "quickstart.html#example-response-abridged",
    "href": "quickstart.html#example-response-abridged",
    "title": "LLM Quick Start",
    "section": "Example Response (abridged)",
    "text": "Example Response (abridged)\n{\n  \"choices\": [{\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"The moon does not have a capital. It is not inhabited or governed.\",\n    },\n    \"finish_reason\": \"stop\"\n  }],\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"completion_tokens\": 12,\n    \"total_tokens\": 21,\n    \"completion_tokens_details\": {\n      \"reasoning_tokens\": 0\n    }\n  }\n}"
  },
  {
    "objectID": "quickstart.html#example-request-1",
    "href": "quickstart.html#example-request-1",
    "title": "LLM Quick Start",
    "section": "Example Request",
    "text": "Example Request\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a terse assistant.\"},\n      {\"role\": \"user\", \"content\": \"What is the capital of the moon?\"},\n      {\"role\": \"assistant\", \"content\": \"The moon does not have a capital. It is not inhabited or governed.\"},\n      {\"role\": \"user\", \"content\": \"Are you sure?\"}\n    ]\n}'"
  },
  {
    "objectID": "quickstart.html#example-response-abridged-1",
    "href": "quickstart.html#example-response-abridged-1",
    "title": "LLM Quick Start",
    "section": "Example Response (abridged)",
    "text": "Example Response (abridged)\n{\n  \"choices\": [{\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"Yes, I am sure. The moon has no capital or formal governance.\"\n    },\n    \"finish_reason\": \"stop\"\n  }],\n  \"usage\": {\n    \"prompt_tokens\": 52,\n    \"completion_tokens\": 15,\n    \"total_tokens\": 67,\n    \"completion_tokens_details\": {\n      \"reasoning_tokens\": 0\n    }\n  }\n}"
  },
  {
    "objectID": "quickstart.html#tokens",
    "href": "quickstart.html#tokens",
    "title": "LLM Quick Start",
    "section": "Tokens",
    "text": "Tokens\n\n\nFundamental units of information for LLMs\nWords, parts of words, or individual characters\n\n“hello” → 1 token\n“unconventional” → 3 tokens: un|con|ventional\n4K video frame at full res → 6885 tokens\n\nImportant for:\n\nModel input/output limits\nAPI pricing is usually by token (see calculator)"
  },
  {
    "objectID": "quickstart.html#instructions",
    "href": "quickstart.html#instructions",
    "title": "LLM Quick Start",
    "section": "Instructions",
    "text": "Instructions\nOpen and run one of these options:\n\n01-basics.R\n01-basics-openai.py (low level library)\n01-basics-langchain.py (high level framework)\n01-basics-chatlas.py (high level framework, similar to R)\n\nIf it errors, now is the time to debug.\nIf it works, study the code and try to understand how it maps to the low-level HTTP descriptions we just went through."
  },
  {
    "objectID": "quickstart.html#summary",
    "href": "quickstart.html#summary",
    "title": "LLM Quick Start",
    "section": "Summary",
    "text": "Summary\n\nA message is an object with a role (“system”, “user”, “assistant”) and a content string\nA chat conversation is a growing list of messages\nThe OpenAI chat API is a stateless HTTP endpoint: takes a list of messages as input, returns a new message as output"
  },
  {
    "objectID": "quickstart.html#shiny-for-r",
    "href": "quickstart.html#shiny-for-r",
    "title": "LLM Quick Start",
    "section": "Shiny for R",
    "text": "Shiny for R\n{shinychat} package\nhttps://github.com/jcheng5/shinychat\n\nDesigned to be used with ellmer\nEllmer Assistant is quite good for getting started"
  },
  {
    "objectID": "quickstart.html#shiny-for-python",
    "href": "quickstart.html#shiny-for-python",
    "title": "LLM Quick Start",
    "section": "Shiny for Python",
    "text": "Shiny for Python\nUse the official ui.Chat component for Shiny for Python\n\nAsk Shiny assistant for help\nDo NOT use the older ChatStream component"
  },
  {
    "objectID": "quickstart.html#other-python-frameworks",
    "href": "quickstart.html#other-python-frameworks",
    "title": "LLM Quick Start",
    "section": "Other Python frameworks",
    "text": "Other Python frameworks\n\nStreamlit has an excellent chat component with a nice LangChain integration\nGradio has a chat component that is extremely easy to use"
  },
  {
    "objectID": "quickstart.html#what-is-tool-calling",
    "href": "quickstart.html#what-is-tool-calling",
    "title": "LLM Quick Start",
    "section": "What is Tool Calling?",
    "text": "What is Tool Calling?\n\n\nAllows LLMs to interact with other systems\nSounds complicated? It isn’t!\nSupported by most of the newest LLMs, but not all"
  },
  {
    "objectID": "quickstart.html#how-it-works",
    "href": "quickstart.html#how-it-works",
    "title": "LLM Quick Start",
    "section": "How It Works",
    "text": "How It Works\n\n\nNot like this - with the assistant executing stuff\nYes like this\n\nUser asks assistant a question; includes metadata for available tools\nAssistant asks the user to invoke a tool, passing its desired arguments\nUser invokes the tool, and returns the output to the assistant\nAssistant incorporates the tool’s output as additional context for formulating a response"
  },
  {
    "objectID": "quickstart.html#how-it-works-1",
    "href": "quickstart.html#how-it-works-1",
    "title": "LLM Quick Start",
    "section": "How It Works",
    "text": "How It Works\nAnother way to think of it:\n\nThe client can perform tasks that the assistant can’t do\nTools put control into the hands of the assistant—it decides when to use them, and what arguments to pass in, and what to do with the results\nHaving an “intelligent-ish” coordinator of tools is a surprisingly general, powerful capability!"
  },
  {
    "objectID": "quickstart.html#openai-models",
    "href": "quickstart.html#openai-models",
    "title": "LLM Quick Start",
    "section": "OpenAI models",
    "text": "OpenAI models\n\nGPT-4o: best general purpose model\nGPT-4o-mini: similar to 4o, but faster and cheaper (and dumber)\no1: reasoning model; better for complex math and coding, but much slower and more expensive\no3-mini: reasoning model; faster and cheaper than o1, doesn’t support vision\nCan access API via OpenAI or Azure"
  },
  {
    "objectID": "quickstart.html#anthropic-models",
    "href": "quickstart.html#anthropic-models",
    "title": "LLM Quick Start",
    "section": "Anthropic models",
    "text": "Anthropic models\n\nClaude 3.5 Sonnet: best model for code generation\nClaude 3.5 Haiku: faster, cheaper (and dumber) than Sonnet\nCan access API via Anthropic or AWS Bedrock (instructions for using Bedrock at Posit)"
  },
  {
    "objectID": "quickstart.html#llama-models",
    "href": "quickstart.html#llama-models",
    "title": "LLM Quick Start",
    "section": "Llama models",
    "text": "Llama models\n\nOpen weights: you can download the model\nCan run locally, for example with Ollama\nLlama 3.1 405b: text, 229GB\nLlama 3.3 70b: text, 43GB (perf similar to 3.1 405b)\nLlama 3.2 90b: text+vision, 55GB\nLlama 3.2 11b: text+vision, 7.9GB (can run comfortably on Macbook)\nLlama 3.2 3b, 1b: text, 2GB, 0.8GB\nCan also access these models via API with OpenRouter, Groq, AWS Bedrock"
  },
  {
    "objectID": "quickstart.html#deepseek-models",
    "href": "quickstart.html#deepseek-models",
    "title": "LLM Quick Start",
    "section": "DeepSeek models",
    "text": "DeepSeek models\n\nOpen weights\nCan access via API from DeepSeek, OpenRouter\nCan run locally, for example with Ollama\nDeepSeek R1 671b: 404GB, uses chain of thought (claimed similar perf to OpenAI o1)\nDeepSeek R1 32b, 70b: 20GB, 43GB (claimed similar perf to OpenAI o1-mini)\nSmaller sizes of R1 also available\nDeepSeek V3 671b: 404GB (claimed similar perf to OpenAI 4o, Claude 3.5 Sonnet)"
  },
  {
    "objectID": "quickstart.html#prompt-engineering-directing-behavioroutput",
    "href": "quickstart.html#prompt-engineering-directing-behavioroutput",
    "title": "LLM Quick Start",
    "section": "Prompt engineering: Directing behavior/output",
    "text": "Prompt engineering: Directing behavior/output\n\n“Respond with just the minimal information necessary.”\n“Explain your response in detail.”\n“Think through this step-by-step.”\n“Carefully read and follow these instructions…”\n“If the user asks a question related to data processing, produce R code to accomplish that task.”\n“Be careful to only provide answers that you are sure about. If you are uncertain about an answer, say so.”"
  },
  {
    "objectID": "quickstart.html#prompt-engineering-using-examples-to-guide-behavior",
    "href": "quickstart.html#prompt-engineering-using-examples-to-guide-behavior",
    "title": "LLM Quick Start",
    "section": "Prompt engineering: Using examples to guide behavior",
    "text": "Prompt engineering: Using examples to guide behavior\n\nGoal: Extract ingredient list from recipe and return in a structured format.\nExample user input:\nIn a large bowl, cream together 1 cup of softened unsalted butter and ½ cup of white sugar until smooth. Beat in 1 egg and 1 teaspoon of vanilla extract. Gradually stir in 2 cups of all-purpose flour until the dough forms. Finally, fold in 1 cup of semisweet chocolate chips. Drop spoonfuls of dough onto an ungreased baking sheet and bake at 350°F (175°C) for 10-12 minutes, or until the edges are lightly browned. Let the cookies cool on the baking sheet for a few minutes before transferring to a wire rack to cool completely. Enjoy!"
  },
  {
    "objectID": "quickstart.html#system-prompt-1-just-instructions",
    "href": "quickstart.html#system-prompt-1-just-instructions",
    "title": "LLM Quick Start",
    "section": "System prompt 1: just instructions",
    "text": "System prompt 1: just instructions\nThe user input contains a recipe. Extract a list of ingredients and return it in JSON format.\nAssistant response:\n{\n  \"ingredients\": [\n    \"1 cup of softened unsalted butter\",\n    \"½ cup of white sugar\",\n    \"1 egg\",\n    \"1 teaspoon of vanilla extract\",\n    \"2 cups of all-purpose flour\",\n    \"1 cup of semisweet chocolate chips\"\n  ]\n}"
  },
  {
    "objectID": "quickstart.html#system-prompt-2-detailed-description-of-output",
    "href": "quickstart.html#system-prompt-2-detailed-description-of-output",
    "title": "LLM Quick Start",
    "section": "System prompt 2: detailed description of output",
    "text": "System prompt 2: detailed description of output\nThe user input contains a recipe. Extract a list of ingredients and return it in JSON format. It should be an array of objects, where each object has keys `ingredient`, `quantity`, and `unit`. Put each object on one line of output.\nAssistant response:\n[\n    {\"ingredient\": \"unsalted butter\", \"quantity\": 1, \"unit\": \"cup\"},\n    {\"ingredient\": \"white sugar\", \"quantity\": 1/2, \"unit\": \"cup\"},\n    {\"ingredient\": \"egg\", \"quantity\": 1, \"unit\": \"large\"},\n    {\"ingredient\": \"vanilla extract\", \"quantity\": 1, \"unit\": \"teaspoon\"},\n    {\"ingredient\": \"all-purpose flour\", \"quantity\": 2, \"unit\": \"cups\"},\n    {\"ingredient\": \"semisweet chocolate chips\", \"quantity\": 1, \"unit\": \"cup\"}\n]"
  },
  {
    "objectID": "quickstart.html#system-prompt-3-one-shot-example",
    "href": "quickstart.html#system-prompt-3-one-shot-example",
    "title": "LLM Quick Start",
    "section": "System prompt 3: one-shot example",
    "text": "System prompt 3: one-shot example\nThe user input contains a recipe. Extract a list of ingredients and return it in JSON format.\n\nExample Output:\n\n```json\n[\n    { \"ingredient\": \"Flour\", \"quantity\": 1, \"unit\": \"cup\" },\n    { \"ingredient\": \"Vegetable oil\", \"quantity\": 0.5, \"unit\": \"tsp\" },\n    { \"ingredient\": \"Onion\", \"quantity\": 1, \"unit\": null },\n]\n```\nAssistant response:\n[\n    { \"ingredient\": \"Unsalted butter\", \"quantity\": 1, \"unit\": \"cup\" },\n    { \"ingredient\": \"White sugar\", \"quantity\": 0.5, \"unit\": \"cup\" },\n    { \"ingredient\": \"Egg\", \"quantity\": 1, \"unit\": null },\n    { \"ingredient\": \"Vanilla extract\", \"quantity\": 1, \"unit\": \"teaspoon\" },\n    { \"ingredient\": \"All-purpose flour\", \"quantity\": 2, \"unit\": \"cups\" },\n    { \"ingredient\": \"Semisweet chocolate chips\", \"quantity\": 1, \"unit\": \"cup\" }\n]"
  },
  {
    "objectID": "quickstart.html#adding-contextknowledge-to-prompt",
    "href": "quickstart.html#adding-contextknowledge-to-prompt",
    "title": "LLM Quick Start",
    "section": "Adding context/knowledge to prompt",
    "text": "Adding context/knowledge to prompt\n\nAdd documentation files to prompt\nAdd positive examples (negative examples don’t work well)\nDocs must fit in context window\nExamples\n\nEllmer assistant uses README files in prompt\nSidebot\nFastHTML LLM prompt"
  },
  {
    "objectID": "quickstart.html#rag-retrieval-augmented-generation",
    "href": "quickstart.html#rag-retrieval-augmented-generation",
    "title": "LLM Quick Start",
    "section": "RAG: Retrieval Augmented Generation",
    "text": "RAG: Retrieval Augmented Generation\n\nUseful when documents don’t fit into context window\nSteps:\n\nUser sends query to app: “How do I …?”\nApp retrieves relevant chunks of text via search\nApp sends text and query to LLM\n\n&lt;chunk 1&gt;, &lt;chunk 2&gt;, &lt;chunk 3&gt;. How do I …?\n\nLLM responds with answer\n\nSearch method typically a semantic instead of keyword search, using vector DB\nLLM will only know about chunks that were retrieved; does not “know” entire corpus\nIn general, prompt engineering works better, if docs fit in context window"
  },
  {
    "objectID": "quickstart.html#fine-tuning",
    "href": "quickstart.html#fine-tuning",
    "title": "LLM Quick Start",
    "section": "Fine tuning",
    "text": "Fine tuning\n\nUpdate weights for an existing model with new information\nNot all models can be fine-tuned\nData must be provided in chat conversation format, with query and response\n\nCan’t just feed it documents – this makes fine-tuning more difficult in practice\n\nSupposedly not very effective unless you have a lot of training data"
  },
  {
    "objectID": "quickstart.html#more-on-prompting-rag-fine-tuning",
    "href": "quickstart.html#more-on-prompting-rag-fine-tuning",
    "title": "LLM Quick Start",
    "section": "More on prompting, RAG, fine tuning",
    "text": "More on prompting, RAG, fine tuning\n\nTry prompting first, before RAG and fine tuning.\nOpenAI’s prompt engineering guide\nFine-tuning vs. RAG article"
  },
  {
    "objectID": "quickstart.html#using-posit.cloud",
    "href": "quickstart.html#using-posit.cloud",
    "title": "LLM Quick Start",
    "section": "Using posit.cloud",
    "text": "Using posit.cloud\n\nPosit.cloud workspace with some examples\n\nContains examples from https://github.com/jcheng5/llm-quickstart"
  },
  {
    "objectID": "quickstart.html#going-beyond-chat",
    "href": "quickstart.html#going-beyond-chat",
    "title": "LLM Quick Start",
    "section": "Going beyond chat",
    "text": "Going beyond chat\n\nStructured output can be easily consumed by code: JSON, YAML, CSV, etc.\nUnstructured output cannot: text, images, etc.\n\nLLMs are good at generating unstructured output, but with a little effort, you can get structured output as well."
  },
  {
    "objectID": "quickstart.html#several-techniques-choose-one",
    "href": "quickstart.html#several-techniques-choose-one",
    "title": "LLM Quick Start",
    "section": "Several techniques (choose one)",
    "text": "Several techniques (choose one)\n\nPost-processing: Use a regular expression to extract structured data from the unstructured output (e.g. /```json\\n(.*?)\\n```/)\nSystem prompt: Simply ask the LLM to output structured data. Be clear about what specific format you want, and provide examples—it really helps!\nStructured Output: GPT-4o and GPT-4o-mini now have a first-class Structured Output feature: outputs strictly adhere to a JSON schema you write. (Docs: openai, LangChain)\nTool calling: Create a tool to receive your output, e.g., set_result(object), where its implementation sets some variable. (Works great for ellmer.)\nLangChain: Has its own abstractions for parsing unstructured output\n\nAsk #hackathon-12 for help if you’re stuck! (Or ask ChatGPT/Claude to make an example.)"
  },
  {
    "objectID": "quickstart.html#using-images-as-input",
    "href": "quickstart.html#using-images-as-input",
    "title": "LLM Quick Start",
    "section": "Using images as input",
    "text": "Using images as input\n\nModern models are pretty good at this\nCan understand both photographs and plots\nExamples for R and Chatlas are in your repo as 05-vision*\nSee docs for LangChain multimodal, OpenAI vision"
  },
  {
    "objectID": "quickstart.html#past-cohort-projects",
    "href": "quickstart.html#past-cohort-projects",
    "title": "LLM Quick Start",
    "section": "Past cohort projects",
    "text": "Past cohort projects\n\nCohort 1\nCohort 2\nCohort 3\nCohort 4\nCohort 5\nCohort 6\nCohort 7\nCohort 8\nCohort 9\nCohort 10"
  },
  {
    "objectID": "quickstart.html#interesting-tools-that-dont-require-coding",
    "href": "quickstart.html#interesting-tools-that-dont-require-coding",
    "title": "LLM Quick Start",
    "section": "Interesting tools that don’t require coding",
    "text": "Interesting tools that don’t require coding\n\nGoogle NotebookLM\nClaude Projects (requires Claude subscription)"
  },
  {
    "objectID": "intro.html#whos-here",
    "href": "intro.html#whos-here",
    "title": "AI Hackathon",
    "section": "Who’s Here",
    "text": "Who’s Here\n\n\nParticipants:\n\nAndrie de Vries\nDavid McGaffin\nKristin Bott\nMariano Caputto\nRika Gorn\nSarah Choi\nTom Mock\nZack Verham\n\n\nOrganizers:\n\nAndrew Holz\nJoe Cheng\nWinston Chang"
  },
  {
    "objectID": "intro.html#your-turn",
    "href": "intro.html#your-turn",
    "title": "AI Hackathon",
    "section": "Your Turn",
    "text": "Your Turn\n\nWhat do you do at Posit?\nHow have you used LLMs/AI tools up until now?\nWhat is your skepticism/enthusiasm score (1 to 5)?"
  },
  {
    "objectID": "intro.html#the-plan",
    "href": "intro.html#the-plan",
    "title": "AI Hackathon",
    "section": "The Plan",
    "text": "The Plan\n\n\n\n\n\nNow: Quick Start course on LLMs. You will leave having used a Chat API.\nNext 48 hours: Hack on stuff! Minimum four hours of effort. “Rules” on the next slide.\nThursday: Show & tell, share lessons learned, reflections.\nPost-hackathon:\n\nKeep hacking! (optional)\nThink about how AI might apply to your team\nBe a resource for others around you"
  },
  {
    "objectID": "intro.html#hack-on-stuff",
    "href": "intro.html#hack-on-stuff",
    "title": "AI Hackathon",
    "section": "Hack On Stuff",
    "text": "Hack On Stuff\n\nDoes NOT have to be relevant to your day job, or Posit, or data science.\nThis exercise is about learning and engagement, not ROI.\nDoes NOT have to be a finished product/demo/app/API.\nShowing some things you did in a notebook is fine as long as YOU found it interesting.\nDoes NOT have to use Posit products.\nYou may use any framework, any language,any service that you have access to."
  },
  {
    "objectID": "intro.html#hack-on-stuff-1",
    "href": "intro.html#hack-on-stuff-1",
    "title": "AI Hackathon",
    "section": "Hack On Stuff",
    "text": "Hack On Stuff\n\nDoes NOT have to be an original idea.\nYou can build on existing projects, improve on existing demos, etc.\nDoes NOT even have to be coding.\nDo a deep dive into an AI service or piece of software. Make an interesting project on ChatGPT Playground &gt; Assistants, or assemble a useful NotebookLM and see what its limits are.\nDoes NOT have to be a success.\nNegative results (“I thought LLMs could do this but turns out they can’t”) are useful results as well. But please be prepared to talk about what you tried."
  },
  {
    "objectID": "intro.html#not-a-contest",
    "href": "intro.html#not-a-contest",
    "title": "AI Hackathon",
    "section": "Not a contest",
    "text": "Not a contest\n\nThis is not a competition. There are no prizes, no judging.\nEveryone is coming from different backgrounds and has different levels of experience with AI, with coding, etc.\nThe goal is to learn, to have fun, and to engage with the technology—and share what you learned with others (and not just within this cohort)."
  },
  {
    "objectID": "intro.html#let-it-rip",
    "href": "intro.html#let-it-rip",
    "title": "AI Hackathon",
    "section": "Let It Rip",
    "text": "Let It Rip\n\nAll that said… also feel free to throw down, and make something super cool!\n\n\n\nFour hours is the minimum, not the maximum—if you’re having fun, keep going!"
  },
  {
    "objectID": "intro.html#a-caveat-from-it",
    "href": "intro.html#a-caveat-from-it",
    "title": "AI Hackathon",
    "section": "A Caveat From IT",
    "text": "A Caveat From IT\nNo proprietary code or data is allowed to be sent to any LLM service, with the below exceptions.\n\nGoogle NotebookLM is allowed.\nAWS Bedrock is allowed, and it has Anthropic models. (Instructions)\nRunning any local model is allowed, we can help you with this as well.\nSending open source code to any service is allowed."
  },
  {
    "objectID": "all-projects.html",
    "href": "all-projects.html",
    "title": "Hackathon projects",
    "section": "",
    "text": "Cohort\nAuthor\nDescription\nKeywords\n\n\n\n\n1\nJonathan Keane\nImage description app\nLangChain, Vision\n\n\n1\nJonathan Yoder\nAutomated R source code analyzer\nClaude, Code Analysis\n\n\n1\nHassan Kibirige\nQueen Discography Trivia game\n\n\n\n1\nAmanda Gadrow\nPosit configuration file generator\n\n\n\n1\nMax Kuhn\nR console assistant that answers questions about tidymodels\ngpt-4-mini, Anthropic\n\n\n1\nHadley Wickham\nProject using RAG for better answers than ChatGPT\nRAG\n\n\n1\nKristin Bott\nExploration of using LLMs to help users investigate their own data\nElmer Assistant\n\n\n1\nKelly O’Briant\nChat app that answers questions about a Connect server using the Connect API\nConnect API\n\n\n1\nHadley Wickham\nProject to convert epub recipe books to density-based measurements\n\n\n\n2\nElliot Murphy\nShiny for Python app for labeling GitHub issues\nShiny, Azure OpenAI, Llama\n\n\n2\nJenny Bryan\nSearchable recipe JSON database from Fine Cooking magazine PDFs\nClaude, Vision, JSON, Shiny\n\n\n2\nIsabella Velázquez\nShiny app that generates ALT text for plots\nShiny, ellmer-assistant\n\n\n2\nSimon Couch\nR package using ellmer and Claude to help migrate from stop()/abort() calls to cli_abort()\nellmer, Claude\n\n\n2\nGeorge Stagg\nLLM-powered code linter for Positron\n\n\n\n2\nAaron Jacobs\nExploration of using e{l}mer to interact with Snowflake’s Cortex AI\ne{l}mer, Snowflake\n\n\n2\nGarrett Grolemund\nShiny app that constructs and grades coding exercises\nShiny, GPT-40\n\n\n2\nToni Noble\nGithub Assistant that answers user questions in Slack\nClaude\n\n\n3\nCarson Sievert\nR package that uses a Shiny chatbot to help users explore a dataset\n\n\n\n3\nRich Iannone\nNew York Times news feed with article summaries and location visualization\nGPT-40-mini\n\n\n3\nTino Ntentes\nFunction to generate and execute regexes based on user description\nGPT-40, Claude-3.5-sonnet\n\n\n3\nBrian Vivas\nChatbot that queries and returns data from the Web Soil Survey\nGPT-40-mini\n\n\n3\nEric Nantz\nChat sidebar that filters and queries a podcast database\nGPT-40-mini\n\n\n3\nEric Nantz\n“Haunted Places” Shiny quiz app\nGPT-40-mini, Shiny\n\n\n3\nEmil Hvitfeldt\n{pal} extension to create a new Quarto reveals theme\nClaude-3.5-sonnet\n\n\n3\nCurtis Kephart\nShiny app for exploring World Happiness Survey data\nGPT-40-mini, Shiny\n\n\n3\nNihara Thomas\nChatbot that answers user questions using Posit Connect API documentation\ngpt-40, Claude-3.5-sonnet\n\n\n3\nMaria Semple\nDungeons & Dragons Shiny app with conversational input/output\n\n\n\n4\nHannah Frick\n{pal} extension to convert R Markdown/knitr to Quarto\n{pal}\n\n\n4\nRachael Dempsey\nNotebookLM project to query information from Data Science Hangout recordings\nNotebookLM\n\n\n4\nTaylor Hoover\nProject using ellmer to run a local LLM (ollama with llama 3.1) and execute tool calling\nellmer, llama\n\n\n4\nMatt Lands\nMulti-agent project using CrewAI.com to analyze stock data and news\nGPT-40\n\n\n4\nAlex Chisholm\nProject to generate tabular datasets and R packages using ChatGPT\nChatGPT\n\n\n4\nClay Raynor\nConsole app that functions as a rhyming dictionary and a reverse dictionary\n\n\n\n5\nRandy Zwitch\nShiny app using GPT-40 to control the Fusion360 API\nShiny, GPT-40\n\n\n5\nKyle Hekhuis\nInvestigation of methods for bypassing OpenAI content censoring\n\n\n\n5\nJeremy Allen\nPlanned AI app to explain social situations from photos or text descriptions\nVision\n\n\n5\nGordon Woodhull\nChatbot that formats its response as a Quarto document\n\n\n\n5\nSam Edwards\nZendesk support ticket autoresponder, experimenting with Bedrock and embedding.io\nAnthropic-on-AWS, embedding.io\n\n\n5\nNeal Richardson\nConnect administration chatbot using various methods, including RAG\nRAG, Claude\n\n\n5\nKaran Gathani\nShiny Help Desk chatbot that automatically labels and responds to GitHub issues\nClaude, Shiny Assistant, GPT-40-mini\n\n\n6\nJordan Jensen\nCode review project using Claude\nClaude\n\n\n6\nAsh Allen\nCommand-line interface for natural language search of the CVE database\n\n\n\n6\nMichael Beigelmacher\nChatbot using RAG and vector databases\nRAG\n\n\n6\nGraham H\nProject using LLMs to generate tags from a document with location information\n\n\n\n6\nEmily N\nProject to generate knitting patterns using GPT-40, RAG, embedding.io, and NotebookLM\nGPT-40, RAG, embedding.io, NotebookLM\n\n\n6\nNick S\nCode translation app that uses environment information for improved accuracy\nVercel AI SDK\n\n\n6\nTim M\nShiny app with a voice interface that visualizes streaming TV data\nShiny\n\n\n6\nJames Blair\nAI-powered exercise coach for cyclists\n\n\n\n7\nJimmy Lubega\nAttempted web app for discussing stocks with stock data fetching and LLM-assisted coding\nClaude\n\n\n7\nVeronica Clarke\nAI companion app that helps users tailor their messages to different audiences\nChatGPT\n\n\n7\nDaniel Chen\nRecipe card generator that uses an Instagram reel link\n\n\n\n7\nFerit Albukrek\nConnect chatbot using Claude, Bedrock, and the Connect SDK\nClaude, Bedrock, Connect SDK\n\n\n7\nTaylor Hoover\nOrg chart generator from text descriptions of job responsibilities\n\n\n\n7\nAron Atkins\nEarworm chatbot that attempts to play song melodies\nGPT-40-mini, Claude 3.5 Sonnet\n\n\n8\nBrian Deitte\nProject to analyze Chronicle data and generate R code using an LLM\n\n\n\n8\nShannon Hagerty\nExploration of SE call transcript data using NotebookLM\nNotebookLM\n\n\n8\nHeath Young\n“Second Brain” creation project using Claude and Quarto\nClaude\n\n\n8\nSam Perman\nProject to determine frameworks used in a repository and select appropriate LLM\n\n\n\n8\nToph Allen\nExploration of methods for using R to call the Connect API\nConnect API\n\n\n8\nRyan Johnson\nChatbot to assist with meal planning and shopping list creation\n\n\n\n8\nMichael Chow\nProject using the Cline VS Code extension to debug a Great Tables issue\nCline\n\n\n8\nKevin Gartland\nAI project to identify blurry images from a photo dump\nchatlas, GPT-40, Vision\n\n\n9\nDavid Aja\nProject using Claude and GPT-40 to create structured data from images of cocktail recipes\nClaude, GPT-40, Vision, Elmer\n\n\n9\nJason Milnes\nShiny app that assists sales reps with LinkedIn prospecting\nChatGPT, Shiny\n\n\n9\nIbi Togun\nProject using LLMs to gather information about Posit competitor usage\n\n\n\n9\nBrian Tackett\nProject testing LLM ability to extrapolate from surname patterns and combine Buffalo city datasets\n\n\n\n9\nBarrett Schloerke\nProject to map Instagram posts using instaloader and Whisper\n\n\n\n9\nIngrid Rodriguez\nDisney vacation planner app using LLMs\n\n\n\n9\nGarrick Aden-Buie\nProject to create brand guideline .yml files for Quarto and Shiny"
  },
  {
    "objectID": "02-tools-prompt.html",
    "href": "02-tools-prompt.html",
    "title": "",
    "section": "",
    "text": "You’re hosting a quiz game show.\n\nBefore you start, ask the user to choose a theme.\nAsk simple questions and ask the user to answer them via multiple choice.\nAfter the user answers, provide feedback and then move on to the next question.\nAfter every 5 questions, declare the user to be a winner regardless of their score, lavish them with praise, and start the game over.\nPlay sound effects for each answer, and when the user ‘wins’.\nEmojis are fun, use them liberally!\n\nExample:\n&lt;Assistant&gt;\n  **Question 3:** What is the center of an atom called?\n\n  A) Electron\n  B) Proton\n  C) Neutron\n  D) Nucleus\n\n  Your answer?\n&lt;/Assistant&gt;\n&lt;User&gt;\n  D\n&lt;/User&gt;\n&lt;Assistant&gt;\n  &lt;ToolCall&gt;\n    {\n      id: \"call_1551214\",\n      function: {\n        arguments: \"{sound: 'correct'}\",\n        name: \"play_sound\"\n      },\n      type: \"function\"\n    }\n  &lt;/ToolCall&gt;\n&lt;/Assistant&gt;\n&lt;User&gt;\n  &lt;ToolResponse&gt;\n    {\n      id: \"call_1551214\",\n      result: {\n        success: true,\n        value: null\n      }\n    }\n  &lt;/ToolResponse&gt;\n&lt;/User&gt;\n&lt;Assistant&gt;\n  Correct! The nucleus is the center of an atom.\n\n  **Question 4:** ...\n&lt;/Assistant&gt;\""
  },
  {
    "objectID": "apps/basic-chat/prompt.html",
    "href": "apps/basic-chat/prompt.html",
    "title": "",
    "section": "",
    "text": "You are an AI assistant designed to help users with questions about data analysis with R. To answer user questions, you can provide code examples in R. Only answer questions related to data analysis and R."
  }
]