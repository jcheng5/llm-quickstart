[
  {
    "objectID": "llm-chat/basic-chat/prompt.html",
    "href": "llm-chat/basic-chat/prompt.html",
    "title": "",
    "section": "",
    "text": "You are an AI assistant designed to help users with questions about data analysis with R. To answer user questions, you can provide code examples in R. Only answer questions related to data analysis and R."
  },
  {
    "objectID": "apps/basic-chat/prompt.html",
    "href": "apps/basic-chat/prompt.html",
    "title": "",
    "section": "",
    "text": "You are an AI assistant designed to help users with questions about data analysis with R. To answer user questions, you can provide code examples in R. Only answer questions related to data analysis and R."
  },
  {
    "objectID": "02-tools-prompt.html",
    "href": "02-tools-prompt.html",
    "title": "",
    "section": "",
    "text": "You’re hosting a quiz game show.\n\nBefore you start, ask the user to choose a theme.\nAsk simple questions and ask the user to answer them via multiple choice.\nAfter the user answers, provide feedback and then move on to the next question.\nAfter every 5 questions, declare the user to be a winner regardless of their score, lavish them with praise, and start the game over.\nPlay sound effects for each answer, and when the user ‘wins’.\nEmojis are fun, use them liberally!\n\nExample:\n&lt;Assistant&gt;\n  **Question 3:** What is the center of an atom called?\n\n  A) Electron\n  B) Proton\n  C) Neutron\n  D) Nucleus\n\n  Your answer?\n&lt;/Assistant&gt;\n&lt;User&gt;\n  D\n&lt;/User&gt;\n&lt;Assistant&gt;\n  &lt;ToolCall&gt;\n    {\n      id: \"call_1551214\",\n      function: {\n        arguments: \"{sound: 'correct'}\",\n        name: \"play_sound\"\n      },\n      type: \"function\"\n    }\n  &lt;/ToolCall&gt;\n&lt;/Assistant&gt;\n&lt;User&gt;\n  &lt;ToolResponse&gt;\n    {\n      id: \"call_1551214\",\n      result: {\n        success: true,\n        value: null\n      }\n    }\n  &lt;/ToolResponse&gt;\n&lt;/User&gt;\n&lt;Assistant&gt;\n  Correct! The nucleus is the center of an atom.\n\n  **Question 4:** ...\n&lt;/Assistant&gt;\""
  },
  {
    "objectID": "intro.html#your-turn",
    "href": "intro.html#your-turn",
    "title": "",
    "section": "Your Turn",
    "text": "Your Turn\n\nYour name, team, and role\nHow have you used LLMs/AI tools up until now?\nWhat is your skepticism/enthusiasm score (1 to 5)?"
  },
  {
    "objectID": "intro.html#the-plan",
    "href": "intro.html#the-plan",
    "title": "",
    "section": "The Plan",
    "text": "The Plan\n\n\nNow: Quick Start course on LLMs. You will leave having used a Chat API.\nNext 48 hours: Hack on stuff! Minimum four hours of effort. “Rules” on the next slide.\n\nWe’ll be continuously monitoring #hackathon-17\n\nThursday: Show & tell, share lessons learned, reflections.\nPost-hackathon:\n\nKeep hacking! (optional)\nThink about how AI might apply to your team\nBe a resource for others around you"
  },
  {
    "objectID": "intro.html#hack-on-stuff",
    "href": "intro.html#hack-on-stuff",
    "title": "",
    "section": "Hack On Stuff",
    "text": "Hack On Stuff\n\n\nDoes NOT have to be relevant to your day job, or Posit, or data science.\nThis exercise is about learning and engagement, not ROI.\nDoes NOT have to be a finished product/demo/app/API.\nShowing some things you did in a notebook is fine as long as YOU found it interesting.\nDoes NOT have to use Posit products.\nYou may use any framework, any language, any service that you have access to."
  },
  {
    "objectID": "intro.html#hack-on-stuff-1",
    "href": "intro.html#hack-on-stuff-1",
    "title": "",
    "section": "Hack On Stuff",
    "text": "Hack On Stuff\n\n\nDoes NOT have to be an original idea.\nYou can build on existing projects, improve on existing demos, etc.\nDoes NOT even have to be coding.\nDo a deep dive into an AI service or piece of software. Create something in Google AI Studio, or assemble a useful NotebookLM, and see what its limits are.\nDoes NOT have to be a success.\nNegative results (“I thought LLMs could do this but turns out they can’t”) are useful results as well. But please be prepared to talk about what you tried."
  },
  {
    "objectID": "intro.html#not-a-contest",
    "href": "intro.html#not-a-contest",
    "title": "",
    "section": "Not a contest",
    "text": "Not a contest\n\nThis is not a competition. There are no prizes, no judging.\nEveryone is coming from different backgrounds and has different levels of experience with AI, with coding, etc.\nThe goal is to learn, to have fun, and to engage with the technology—and share what you learned with others (and not just within this cohort)."
  },
  {
    "objectID": "intro.html#let-it-rip",
    "href": "intro.html#let-it-rip",
    "title": "",
    "section": "Let It Rip",
    "text": "Let It Rip\n\nAll that said… also feel free to throw down, and make something super cool!\n\n\n\nFour hours is the minimum, not the maximum—if you’re having fun, keep going!"
  },
  {
    "objectID": "intro.html#a-caveat-from-it",
    "href": "intro.html#a-caveat-from-it",
    "title": "",
    "section": "A Caveat From IT",
    "text": "A Caveat From IT\nNo proprietary code or data is allowed to be sent to any LLM service, with the below exceptions.\n\nGoogle NotebookLM is allowed.\nAWS Bedrock is allowed, and it has Anthropic models. (Instructions)\nRunning any local model is allowed, we can help you with this as well.\nSending open source code to any service is allowed."
  },
  {
    "objectID": "quickstart.html#setup",
    "href": "quickstart.html#setup",
    "title": "LLM Quick Start",
    "section": "Setup",
    "text": "Setup\n\n\nClone https://github.com/jcheng5/llm-quickstart\nGrab your OpenAI/Anthropic API keys; see the thread in #hackathon-17\nFor R: install.packages(c(\"ellmer\", \"shinychat\", \"dotenv\", \"shiny\", \"paws.common\", \"magick\", \"beepr\"))\nFor Python: pip install -r requirements.txt"
  },
  {
    "objectID": "quickstart.html#framing-llms",
    "href": "quickstart.html#framing-llms",
    "title": "LLM Quick Start",
    "section": "Framing LLMs",
    "text": "Framing LLMs\n\n\nOur focus: Practical, actionable information\n\nOften, just enough knowledge so you know what to search for (or better yet, what to ask an LLM)\n\nWe will treat LLMs as black boxes\nDon’t focus on how they work (yet)\n\nLeads to bad intuition about their capabilities\nBetter to start with a highly empirical approach"
  },
  {
    "objectID": "quickstart.html#llm-conversations-are-http-requests",
    "href": "quickstart.html#llm-conversations-are-http-requests",
    "title": "LLM Quick Start",
    "section": "LLM Conversations are HTTP Requests",
    "text": "LLM Conversations are HTTP Requests\n\n\nEach interaction is a separate HTTP API request\nThe API server is entirely stateless (despite conversations being inherently stateful!)"
  },
  {
    "objectID": "quickstart.html#example-conversation",
    "href": "quickstart.html#example-conversation",
    "title": "LLM Quick Start",
    "section": "Example Conversation",
    "text": "Example Conversation\n\n“What’s the capital of the moon?”\n\n\"There isn't one.\"\n\n“Are you sure?”\n\n\"Yes, I am sure.\""
  },
  {
    "objectID": "quickstart.html#example-request",
    "href": "quickstart.html#example-request",
    "title": "LLM Quick Start",
    "section": "Example Request",
    "text": "Example Request\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a terse assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of the moon?\"}\n    ]\n}'\n\nSystem prompt: behind-the-scenes instructions and information for the model\nUser prompt: a question or statement for the model to respond to"
  },
  {
    "objectID": "quickstart.html#example-response-abridged",
    "href": "quickstart.html#example-response-abridged",
    "title": "LLM Quick Start",
    "section": "Example Response (abridged)",
    "text": "Example Response (abridged)\n{\n  \"choices\": [{\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"The moon does not have a capital. It is not inhabited or governed.\",\n    },\n    \"finish_reason\": \"stop\"\n  }],\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"completion_tokens\": 12,\n    \"total_tokens\": 21,\n    \"completion_tokens_details\": {\n      \"reasoning_tokens\": 0\n    }\n  }\n}"
  },
  {
    "objectID": "quickstart.html#example-request-1",
    "href": "quickstart.html#example-request-1",
    "title": "LLM Quick Start",
    "section": "Example Request",
    "text": "Example Request\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a terse assistant.\"},\n      {\"role\": \"user\", \"content\": \"What is the capital of the moon?\"},\n      {\"role\": \"assistant\", \"content\": \"The moon does not have a capital. It is not inhabited or governed.\"},\n      {\"role\": \"user\", \"content\": \"Are you sure?\"}\n    ]\n}'"
  },
  {
    "objectID": "quickstart.html#example-response-abridged-1",
    "href": "quickstart.html#example-response-abridged-1",
    "title": "LLM Quick Start",
    "section": "Example Response (abridged)",
    "text": "Example Response (abridged)\n{\n  \"choices\": [{\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"Yes, I am sure. The moon has no capital or formal governance.\"\n    },\n    \"finish_reason\": \"stop\"\n  }],\n  \"usage\": {\n    \"prompt_tokens\": 52,\n    \"completion_tokens\": 15,\n    \"total_tokens\": 67,\n    \"completion_tokens_details\": {\n      \"reasoning_tokens\": 0\n    }\n  }\n}"
  },
  {
    "objectID": "quickstart.html#tokens",
    "href": "quickstart.html#tokens",
    "title": "LLM Quick Start",
    "section": "Tokens",
    "text": "Tokens\n\n\nFundamental units of information for LLMs\nWords, parts of words, or individual characters\n\n“hello” → 1 token\n“unconventional” → 3 tokens: un|con|ventional\n4K video frame at full res → 6885 tokens\n\nExample with OpenAI Tokenizer\nImportant for:\n\nModel input/output limits\nAPI pricing is usually by token (see calculator)"
  },
  {
    "objectID": "quickstart.html#instructions",
    "href": "quickstart.html#instructions",
    "title": "LLM Quick Start",
    "section": "Instructions",
    "text": "Instructions\nOpen and run one of these options:\n\n01-basics.R\n\nor 01-basics-bedrock.R for cloud\n\n01-basics-openai.py (low level library)\n01-basics-langchain.py (high level framework)\n01-basics-chatlas.py (high level framework, similar to R)\n\nor 01-basics-chatlas-bedrock.py for cloud\n\n\nIf it errors, now is the time to debug.\nIf it works, study the code and try to understand how it maps to the low-level HTTP descriptions we just went through."
  },
  {
    "objectID": "quickstart.html#summary",
    "href": "quickstart.html#summary",
    "title": "LLM Quick Start",
    "section": "Summary",
    "text": "Summary\n\nA message is an object with a role (“system”, “user”, “assistant”) and a content string\nA chat conversation is a growing list of messages\nThe OpenAI chat API is a stateless HTTP endpoint: takes a list of messages as input, returns a new message as output"
  },
  {
    "objectID": "quickstart.html#shiny-for-r",
    "href": "quickstart.html#shiny-for-r",
    "title": "LLM Quick Start",
    "section": "Shiny for R",
    "text": "Shiny for R\n{shinychat} package\nhttps://github.com/jcheng5/shinychat\n\nDesigned to be used with ellmer\nEllmer Assistant is quite good for getting started"
  },
  {
    "objectID": "quickstart.html#shiny-for-python",
    "href": "quickstart.html#shiny-for-python",
    "title": "LLM Quick Start",
    "section": "Shiny for Python",
    "text": "Shiny for Python\nUse the official ui.Chat component for Shiny for Python\n\nAsk Shiny assistant for help\nDo NOT use the older ChatStream component"
  },
  {
    "objectID": "quickstart.html#shiny-assistant-for-vs-code",
    "href": "quickstart.html#shiny-assistant-for-vs-code",
    "title": "LLM Quick Start",
    "section": "Shiny Assistant for VS Code",
    "text": "Shiny Assistant for VS Code"
  },
  {
    "objectID": "quickstart.html#installation",
    "href": "quickstart.html#installation",
    "title": "LLM Quick Start",
    "section": "Installation",
    "text": "Installation"
  },
  {
    "objectID": "quickstart.html#querychat",
    "href": "quickstart.html#querychat",
    "title": "LLM Quick Start",
    "section": "{querychat}",
    "text": "{querychat}\n\nhttps://github.com/posit-dev/querychat (R and Python)"
  },
  {
    "objectID": "quickstart.html#other-python-frameworks",
    "href": "quickstart.html#other-python-frameworks",
    "title": "LLM Quick Start",
    "section": "Other Python frameworks",
    "text": "Other Python frameworks\n\nStreamlit has an excellent chat component with a nice LangChain integration\nGradio has a chat component that is extremely easy to use"
  },
  {
    "objectID": "quickstart.html#what-is-tool-calling",
    "href": "quickstart.html#what-is-tool-calling",
    "title": "LLM Quick Start",
    "section": "What is Tool Calling?",
    "text": "What is Tool Calling?\n\n\nAllows LLMs to interact with other systems\nSounds complicated? It isn’t!\nSupported by most of the newest LLMs, but not all"
  },
  {
    "objectID": "quickstart.html#how-it-works",
    "href": "quickstart.html#how-it-works",
    "title": "LLM Quick Start",
    "section": "How It Works",
    "text": "How It Works\n\n\nNot like this - with the assistant executing stuff\nYes like this\n\nUser asks assistant a question; includes metadata for available tools\nAssistant asks the user to invoke a tool, passing its desired arguments\nUser invokes the tool, and returns the output to the assistant\nAssistant incorporates the tool’s output as additional context for formulating a response"
  },
  {
    "objectID": "quickstart.html#how-it-works-1",
    "href": "quickstart.html#how-it-works-1",
    "title": "LLM Quick Start",
    "section": "How It Works",
    "text": "How It Works\nAnother way to think of it:\n\nThe client can perform tasks that the assistant can’t do\nTools put control into the hands of the assistant—it decides when to use them, and what arguments to pass in, and what to do with the results\nHaving an “intelligent-ish” coordinator of tools is a surprisingly general, powerful capability!"
  },
  {
    "objectID": "quickstart.html#openai-models",
    "href": "quickstart.html#openai-models",
    "title": "LLM Quick Start",
    "section": "OpenAI models",
    "text": "OpenAI models\n\nGPT-4o: good general purpose model\no1: reasoning model; better for complex math and coding, but much slower and more expensive\nAPI access via OpenAI or Azure\nTakeaway: Good models for general purpose use"
  },
  {
    "objectID": "quickstart.html#anthropic-models",
    "href": "quickstart.html#anthropic-models",
    "title": "LLM Quick Start",
    "section": "Anthropic models",
    "text": "Anthropic models\n\nClaude 3.7 Sonnet: good general purpose model, best for code generation\nAPI access via Anthropic or AWS Bedrock (instructions for using Bedrock at Posit)\nTakeaway: Best model for code generation"
  },
  {
    "objectID": "quickstart.html#google-models",
    "href": "quickstart.html#google-models",
    "title": "LLM Quick Start",
    "section": "Google models",
    "text": "Google models\n\nGemini 2.0 Flash: 1 million token context length, very fast\nGemini 2.0 Pro: 2 million token context length, smarter than Flash\nTakeaway: Largest context length - good if you need to provide lots of information"
  },
  {
    "objectID": "quickstart.html#llama-models",
    "href": "quickstart.html#llama-models",
    "title": "LLM Quick Start",
    "section": "Llama models",
    "text": "Llama models\n\nOpen weights: you can download the model\nCan run locally, for example with Ollama\nLlama 3.1 405b: text, 229GB. Not quite as smart as best closed models.\nLlama 3.2 90b: text+vision, 55GB\nLlama 3.2 11b: text+vision, 7.9GB (can run comfortably on Macbook)\nAPI access via OpenRouter, Groq, AWS Bedrock, others\nTakeaway: Good models if you want to keep all information on premises."
  },
  {
    "objectID": "quickstart.html#deepseek-models",
    "href": "quickstart.html#deepseek-models",
    "title": "LLM Quick Start",
    "section": "DeepSeek models",
    "text": "DeepSeek models\n\nOpen weights\nDeepSeek R1 671b: 404GB, uses chain of thought (claimed similar perf to OpenAI o1)\nDeepSeek R1 32b, 70b: 20GB, 43GB. Not actually DeepSeek architecture - said to be significantly worse.\nAPI access via DeepSeek, OpenRouter\nCan run locally, but smaller models aren’t really DeepSeek."
  },
  {
    "objectID": "quickstart.html#prompt-engineering-directing-behavioroutput",
    "href": "quickstart.html#prompt-engineering-directing-behavioroutput",
    "title": "LLM Quick Start",
    "section": "Prompt engineering: Directing behavior/output",
    "text": "Prompt engineering: Directing behavior/output\n\n“Respond with just the minimal information necessary.”\n“Think through this step-by-step.”\n“Carefully read and follow these instructions…”\n“If the user asks a question related to data processing, produce R code to accomplish that task.”\n“Be careful to only provide answers that you are sure about. If you are uncertain about an answer, say so.”"
  },
  {
    "objectID": "quickstart.html#prompt-engineering-using-examples-to-guide-behavior",
    "href": "quickstart.html#prompt-engineering-using-examples-to-guide-behavior",
    "title": "LLM Quick Start",
    "section": "Prompt engineering: Using examples to guide behavior",
    "text": "Prompt engineering: Using examples to guide behavior\n\nGoal: Extract ingredient list from recipe and return in a structured format.\nExample user input:\nIn a large bowl, cream together 1 cup of softened unsalted butter and ½ cup of white sugar until smooth. Beat in 1 egg and 1 teaspoon of vanilla extract. Gradually stir in 2 cups of all-purpose flour until the dough forms. Finally, fold in 1 cup of semisweet chocolate chips. Drop spoonfuls of dough onto an ungreased baking sheet and bake at 350°F (175°C) for 10-12 minutes, or until the edges are lightly browned. Let the cookies cool on the baking sheet for a few minutes before transferring to a wire rack to cool completely. Enjoy!"
  },
  {
    "objectID": "quickstart.html#system-prompt-1-just-instructions",
    "href": "quickstart.html#system-prompt-1-just-instructions",
    "title": "LLM Quick Start",
    "section": "System prompt 1: just instructions",
    "text": "System prompt 1: just instructions\nThe user input contains a recipe. Extract a list of ingredients and return it in JSON format.\nAssistant response:\n{\n  \"ingredients\": [\n    \"1 cup of softened unsalted butter\",\n    \"½ cup of white sugar\",\n    \"1 egg\",\n    \"1 teaspoon of vanilla extract\",\n    \"2 cups of all-purpose flour\",\n    \"1 cup of semisweet chocolate chips\"\n  ]\n}"
  },
  {
    "objectID": "quickstart.html#system-prompt-2-detailed-description-of-output",
    "href": "quickstart.html#system-prompt-2-detailed-description-of-output",
    "title": "LLM Quick Start",
    "section": "System prompt 2: detailed description of output",
    "text": "System prompt 2: detailed description of output\nThe user input contains a recipe. Extract a list of ingredients and return it in JSON format. It should be an array of objects, where each object has keys `ingredient`, `quantity`, and `unit`. Put each object on one line of output.\nAssistant response:\n[\n    {\"ingredient\": \"unsalted butter\", \"quantity\": 1, \"unit\": \"cup\"},\n    {\"ingredient\": \"white sugar\", \"quantity\": 1/2, \"unit\": \"cup\"},\n    {\"ingredient\": \"egg\", \"quantity\": 1, \"unit\": \"large\"},\n    {\"ingredient\": \"vanilla extract\", \"quantity\": 1, \"unit\": \"teaspoon\"},\n    {\"ingredient\": \"all-purpose flour\", \"quantity\": 2, \"unit\": \"cups\"},\n    {\"ingredient\": \"semisweet chocolate chips\", \"quantity\": 1, \"unit\": \"cup\"}\n]"
  },
  {
    "objectID": "quickstart.html#system-prompt-3-one-shot-example",
    "href": "quickstart.html#system-prompt-3-one-shot-example",
    "title": "LLM Quick Start",
    "section": "System prompt 3: one-shot example",
    "text": "System prompt 3: one-shot example\nThe user input contains a recipe. Extract a list of ingredients and return it in JSON format.\n\nExample Output:\n\n```json\n[\n    { \"ingredient\": \"Flour\", \"quantity\": 1, \"unit\": \"cup\" },\n    { \"ingredient\": \"Vegetable oil\", \"quantity\": 0.5, \"unit\": \"tsp\" },\n    { \"ingredient\": \"Onion\", \"quantity\": 1, \"unit\": null },\n]\n```\nAssistant response:\n[\n    { \"ingredient\": \"Unsalted butter\", \"quantity\": 1, \"unit\": \"cup\" },\n    { \"ingredient\": \"White sugar\", \"quantity\": 0.5, \"unit\": \"cup\" },\n    { \"ingredient\": \"Egg\", \"quantity\": 1, \"unit\": null },\n    { \"ingredient\": \"Vanilla extract\", \"quantity\": 1, \"unit\": \"teaspoon\" },\n    { \"ingredient\": \"All-purpose flour\", \"quantity\": 2, \"unit\": \"cups\" },\n    { \"ingredient\": \"Semisweet chocolate chips\", \"quantity\": 1, \"unit\": \"cup\" }\n]"
  },
  {
    "objectID": "quickstart.html#adding-contextknowledge-to-prompt",
    "href": "quickstart.html#adding-contextknowledge-to-prompt",
    "title": "LLM Quick Start",
    "section": "Adding context/knowledge to prompt",
    "text": "Adding context/knowledge to prompt\n\nAdd documentation files to prompt\nAdd positive examples (negative examples don’t work well)\nDocs must fit in context window\nExamples\n\nEllmer assistant uses README files in prompt\nSidebot\nFastHTML LLM prompt"
  },
  {
    "objectID": "quickstart.html#rag-retrieval-augmented-generation",
    "href": "quickstart.html#rag-retrieval-augmented-generation",
    "title": "LLM Quick Start",
    "section": "RAG: Retrieval Augmented Generation",
    "text": "RAG: Retrieval Augmented Generation\n\nUseful when documents don’t fit into context window\nSteps:\n\nUser sends query to app: “How do I …?”\nApp retrieves relevant chunks of text via search\nApp sends text and query to LLM\n\n&lt;chunk 1&gt;, &lt;chunk 2&gt;, &lt;chunk 3&gt;. How do I …?\n\nLLM responds with answer\n\nSearch method typically a semantic instead of keyword search, using vector DB\nLLM will only know about chunks that were retrieved; does not “know” entire corpus\nIn general, prompt stuffing works better, if docs fit in context window"
  },
  {
    "objectID": "quickstart.html#agentic-search",
    "href": "quickstart.html#agentic-search",
    "title": "LLM Quick Start",
    "section": "Agentic search",
    "text": "Agentic search\n\nSimilar to RAG:\n\nExtra information is provided to LLM\n\nDifferent from RAG:\n\nApplication does not search documents and send to LLM along with user prompt\nUser prompt is sent to LLM, then LLM uses a tool to search for relevant documents"
  },
  {
    "objectID": "quickstart.html#fine-tuning",
    "href": "quickstart.html#fine-tuning",
    "title": "LLM Quick Start",
    "section": "Fine tuning",
    "text": "Fine tuning\n\nUpdate weights for an existing model with new information\nNot all models can be fine-tuned\nData must be provided in chat conversation format, with query and response\n\nCan’t just feed it documents – this makes fine-tuning more difficult in practice\n\nSupposedly not very effective unless you have a lot of training data"
  },
  {
    "objectID": "quickstart.html#takeaways",
    "href": "quickstart.html#takeaways",
    "title": "LLM Quick Start",
    "section": "Takeaways",
    "text": "Takeaways\n\nFirst try pompting, then RAG, and then fine tuning.\nOther resources\n\nOpenAI’s prompt engineering guide\nAnthropic’s prompt engineering guide\nFine-tuning vs. RAG article"
  },
  {
    "objectID": "quickstart.html#going-beyond-chat",
    "href": "quickstart.html#going-beyond-chat",
    "title": "LLM Quick Start",
    "section": "Going beyond chat",
    "text": "Going beyond chat\n\nStructured output can be easily consumed by code: JSON, YAML, CSV, etc.\nUnstructured output cannot: text, images, etc.\n\nLLMs are good at generating unstructured output, but with a little effort, you can get structured output as well."
  },
  {
    "objectID": "quickstart.html#several-techniques-choose-one",
    "href": "quickstart.html#several-techniques-choose-one",
    "title": "LLM Quick Start",
    "section": "Several techniques (choose one)",
    "text": "Several techniques (choose one)\n\nPost-processing: Use a regular expression to extract structured data from the unstructured output (e.g. /```json\\n(.*?)\\n```/)\nSystem prompt: Simply ask the LLM to output structured data. Be clear about what specific format you want, and provide examples—it really helps!\nStructured Output: GPT-4o and GPT-4o-mini now have a first-class Structured Output feature: outputs strictly adhere to a JSON schema you write. (Docs: openai, LangChain)\nTool calling: Create a tool to receive your output, e.g., set_result(object), where its implementation sets some variable. (Works great for ellmer.)\nLangChain: Has its own abstractions for parsing unstructured output\n\nAsk #hackathon-17 for help if you’re stuck! (Or ask ChatGPT/Claude to make an example.)"
  },
  {
    "objectID": "quickstart.html#using-images-as-input",
    "href": "quickstart.html#using-images-as-input",
    "title": "LLM Quick Start",
    "section": "Using images as input",
    "text": "Using images as input\n\nModern models are pretty good at this\nCan understand both photographs and plots\nExamples for R and Chatlas are in your repo as 05-vision*\nSee docs for LangChain multimodal, OpenAI vision"
  },
  {
    "objectID": "quickstart.html#past-cohort-projects",
    "href": "quickstart.html#past-cohort-projects",
    "title": "LLM Quick Start",
    "section": "Past cohort projects",
    "text": "Past cohort projects\n\nCohort 1\nCohort 2\nCohort 3\nCohort 4\nCohort 5\nCohort 6\nCohort 7\nCohort 8\nCohort 9\nCohort 10\nCohort 11\nCohort 12\nCohort 13\nCohort 14\nCohort 15\nCohort 16"
  },
  {
    "objectID": "quickstart.html#interesting-tools-that-dont-require-coding",
    "href": "quickstart.html#interesting-tools-that-dont-require-coding",
    "title": "LLM Quick Start",
    "section": "Interesting tools that don’t require coding",
    "text": "Interesting tools that don’t require coding\n\nGoogle NotebookLM\nClaude Projects (requires Claude subscription)"
  },
  {
    "objectID": "apps/tool-chat/prompt.html",
    "href": "apps/tool-chat/prompt.html",
    "title": "",
    "section": "",
    "text": "You are an assistant which can convert units and perform basic arithmetic with units."
  }
]